{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdallah/anaconda3/lib/python3.9/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml(name=\"mnist_784\")\n",
    "\n",
    "X, y = mnist.data, mnist.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split it to train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=20000, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=10000, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train several classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "y_train = y_train.astype(int)\n",
    "y_val = y_val.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier Accuracy: 0.9677\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "rf_predictions = rf_classifier.predict(X_val)\n",
    "rf_accuracy = accuracy_score(y_val, rf_predictions)\n",
    "print(\"Random Forest Classifier Accuracy:\", rf_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra-Trees Classifier Accuracy: 0.9689\n"
     ]
    }
   ],
   "source": [
    "# Extra-Trees Classifier\n",
    "et_classifier = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "et_classifier.fit(X_train, y_train)\n",
    "et_predictions = et_classifier.predict(X_val)\n",
    "et_accuracy = accuracy_score(y_val, et_predictions)\n",
    "print(\"Extra-Trees Classifier Accuracy:\", et_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First i tried SVC but i took forever to run so i chose XGBost instead which way faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier Accuracy: 0.9348\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# XGBoost Classifier\n",
    "xgb_classifier = xgb.XGBClassifier(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=42)\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "xgb_predictions = xgb_classifier.predict(X_val)\n",
    "xgb_accuracy = accuracy_score(y_val, xgb_predictions)\n",
    "print(\"XGBoost Classifier Accuracy:\", xgb_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We create a soft voting ensemble with the pre-trained classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble (Soft Voting) Accuracy: 0.9613\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "ensemble = VotingClassifier(estimators=[\n",
    "    ('random_forest', rf_classifier),\n",
    "    ('extra_trees', et_classifier),\n",
    "    ('xgboost', xgb_classifier)\n",
    "], voting='soft')\n",
    "\n",
    "# Train the ensemble model on the training data\n",
    "ensemble.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "ensemble_predictions = ensemble.predict(X_val)\n",
    "\n",
    "# Calculate accuracy on the validation set\n",
    "ensemble_accuracy = accuracy_score(y_val, ensemble_predictions)\n",
    "print(\"Ensemble (Soft Voting) Accuracy:\", ensemble_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unexpectedly the accuracy has dropped down  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
